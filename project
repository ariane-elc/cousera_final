library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)
library(plyr)
install.packages('rattle')
#load the data
train_data<-read.csv(file="pml-training.csv")
test_data<-read.csv(file="pml-testing.csv")

# now let's split the testing data into training and testing

set.seed(12345)

exercise_train<-createDataPartition(train_data$classe, p=0.7, list=FALSE)
train=train[exercise_train, ]
testing=train[-exercise_train, ]
dim(training)
dim(testing)

#clean the data
class(train_data)
dim(train_data)
summary(train_data)
  #the first column is non revelant
train=train[c(-1)]
testing=testing[c(-1)]

#there's a lot of "empty" variable, let's put away those with NA
train=train_data[,colSums(is.na(train_data))==0]
final_testing=test_data[,colSums(is.na(test_data))==0]

train <- train[, -c(1:7)]
final_testing <- final_testing[, -c(1:7)]
dim(train)






dim(training)
str(training)

dim(final_testing)
str(final_testing)
  #we need to remove variable of no interest such as the one with near zero variance

nzv=nearZeroVar(train)
train<-train[,-nzv]

testing<-testing[,-nzv]
dim(train)
dim(testing)
dim(final_testing)

dim(final_testing)
str(train)
str(final_testing)
#decision tree

set.seed(12345)
model_rf <- randomForest(classe ~ ., data=train)
prediction_rf <- predict(model_rf, testing, type = "class")
cmrf <- confusionMatrix(prediction_rf, testing$classe)
cmrf

plot(model_rf)
model_rf$confusion
varImpPlot(model_rf)
model_rf$importance[order(model_rf$importance[, 1], decreasing = TRUE), ]

#two main criteria are cvtd timestamp and raw timestamp

plot(classe ~ cvtd_timestamp, data = testing)
plot(classe ~ num_window, data = testing)

#we see stabilisation at 3000
model_rf <- randomForest(classe ~ ., data=training, ntree=3000, mtry=2, na.action=na.roughfix)
print(model_rf)

#now let's try directly with caret library

set.seed(123)
library(caret)
model_caret <- train(classe ~ ., data = training, method = "rf")
print(model_caret)
print(model_caret$finalModel)
varImpPlot(model_caret$finalModel)

varImp(model_caret)
plot(varImp(model_caret), top = 10)

# now let's try to see how accurate we are with classifiction tree

set.seed(12345)
model_decisionTree <- rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(model_decisionTree)

# we test it on test data

predictTree <- predict(model_decisionTree, testing, type = "class")
model_tree <- confusionMatrix(predictTree, testing$classe)
model_tree

#now let's try the GBM model 

set.seed(12345)
control_GBM <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
model_GBM  <- train(classe ~ ., data=training, method = "gbm", trControl = control_GBM, verbose = FALSE)
model_GBM$finalModel
print(model_GBM)
  #test  the GBM model and accuracy

predict_GBM <- predict(model_GBM, newdata=testing)
cmGBM <- confusionMatrix(predict_GBM, testing$classe)
cmGBM

# now let's apply rf to the final_testing model

str(testing)
str(final_testing)
final_t<-data.frame(final_testing)
Results <- predict(model_rf, newdata=final_t)
Results
